{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "if os.path.abspath(os.path.join(os.getcwd(), '../..')) not in sys.path:\n",
    "    sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../..')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-12 01:36:13.403196: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-12 01:36:13.411564: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-12 01:36:13.413976: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-12 01:36:13.420432: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-12 01:36:13.838132: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from src.objects.actorcriticalgorithm import ActorCriticAlgorithm\n",
    "from src.objects.gameengine import GameEngine\n",
    "from src.objects.actorcritic import ActorCritic\n",
    "from src.objects.grid import Grid\n",
    "from src.objects.snake import Snake\n",
    "from src.utils.utils import create_rabbits\n",
    "from typing import Tuple\n",
    "import collections\n",
    "import numpy as np\n",
    "import keras\n",
    "import statistics\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from PyQt5.QtCore import Qt\n",
    "from src.utils.astar import Astar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def run_episode(\n",
    "    env: GameEngine,\n",
    "    initial_state: tf.Tensor,\n",
    "    model: tf.keras.Model,\n",
    ") -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n",
    "    action_probs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "    values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "    rewards = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
    "    state = initial_state\n",
    "    done = tf.constant(False, dtype=tf.bool)\n",
    "    step_index = 0\n",
    "    tf.autograph.experimental.set_loop_options(\n",
    "        shape_invariants=[(done, tf.TensorShape([])), (state, state.shape)])\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        action_logits_step, value = model(state)\n",
    "        action_logits_step = tf.reshape(action_logits_step, [1, -1])\n",
    "\n",
    "        action = tf.random.categorical(action_logits_step, 1)[0, 0]\n",
    "\n",
    "        action_probs_step = tf.nn.softmax(action_logits_step)\n",
    "        values = values.write(step_index, tf.squeeze(value))\n",
    "\n",
    "        action_probs = action_probs.write(step_index,\n",
    "                                          action_probs_step[0, action])\n",
    "\n",
    "        next_state, reward, done = env.tf_step(action=action)\n",
    "\n",
    "        rewards = rewards.write(step_index, reward)\n",
    "\n",
    "        # if tf.cast(done, tf.bool):\n",
    "        #     break\n",
    "\n",
    "        state = next_state\n",
    "        step_index += 1\n",
    "\n",
    "    action_probs = action_probs.stack()\n",
    "    values = values.stack()\n",
    "    rewards = rewards.stack()\n",
    "    return action_probs, value, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(env: GameEngine, initial_state: tf.Tensor,\n",
    "               model: tf.keras.Model, gamma: float,\n",
    "               optimizer: tf.keras.optimizers.Optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        action_probs, values, rewards = run_episode(\n",
    "            env=env, initial_state=initial_state, model=model)\n",
    "\n",
    "        returns = get_expected_return(rewards=rewards, gamma=gamma)\n",
    "\n",
    "        loss = compute_loss(action_probs, values, returns)\n",
    "        # Appliquer les gradients pour mettre à jour le modèle\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    episode_reward = tf.math.reduce_mean(rewards)\n",
    "    return episode_reward\n",
    "\n",
    "\n",
    "huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
    "\n",
    "\n",
    "def compute_loss(\n",
    "    actions_probs: tf.Tensor,\n",
    "    values: tf.Tensor,\n",
    "    returns: tf.Tensor,\n",
    ") -> tf.Tensor:\n",
    "    \"\"\"Compute the combined actor-critic loss\"\"\"\n",
    "    advantage = returns - values\n",
    "    action_log_probs = tf.math.log(actions_probs)\n",
    "    actor_loss = -tf.math.reduce_sum(action_log_probs * advantage)\n",
    "    critic_loss = huber_loss(values, returns)\n",
    "    return actor_loss + critic_loss\n",
    "\n",
    "\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "\n",
    "def get_expected_return(rewards: tf.Tensor,\n",
    "                        gamma: float,\n",
    "                        standardize: bool = True) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "      Compute expected returns per timestep.\n",
    "        La séquence de récompenses pour chaque pas de temps collecté au cours d'un épisode est convertie en une séquence de rendements attendus dans laquelle la somme des récompenses est prise du pas de temps actuel t à T et chaque fois la récompense est multipliée par un facteur de réduction décroissant de manière exponentielle gamma:\n",
    "        Depuis gamma appartient à l'intervalle [0,1], les récompenses plus éloignées du pas de temps actuel ont moins de poids.\n",
    "        Intuitivement, le rendement attendu implique simplement que les récompenses maintenant sont meilleures que les récompenses plus tard. Au sens mathématique, il s'agit de s'assurer que la somme des récompenses converge.\n",
    "        Pour stabiliser la formation, la séquence résultante des rendements est également standardisée (c'est-à-dire pour avoir une moyenne nulle et un écart-type unitaire).\n",
    "    \"\"\"\n",
    "\n",
    "    n = tf.shape(rewards)[0]\n",
    "    returns = tf.TensorArray(dtype=tf.float32, shape=n)\n",
    "    # Start from the end of `rewards` and accumulate reward sums\n",
    "    # into the `returns` array\n",
    "    rewards = tf.cast(rewards[::-1], dtype=tf.float32)\n",
    "    discounted_sum = tf.constant(0.0)\n",
    "    discounted_sum_shape = discounted_sum.shape\n",
    "    for i in tf.range(n):\n",
    "        reward = rewards[i]\n",
    "        discounted_sum = reward + gamma * discounted_sum\n",
    "        returns = returns.write(i, discounted_sum)\n",
    "    returns = returns.stack()[::-1]\n",
    "    if standardize:\n",
    "        returns = ((returns - tf.math.reduce_mean(returns) /\n",
    "                    (tf.math.reduce_std(returns) + eps)))\n",
    "    return returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "n_directions = 4\n",
    "\n",
    "snake = Snake(body=[(3, 3), (3, 4), (3, 5)], direction=Qt.Key_Up)\n",
    "model = ActorCritic(n_directions)\n",
    "\n",
    "# width, height = random.randint(20, 150), random.randint(20, 100)\n",
    "width, height = 8, 8\n",
    "nb_lapins = 1\n",
    "\n",
    "rabbits = create_rabbits(width=width,\n",
    "                         height=height,\n",
    "                         n_rabbits=nb_lapins,\n",
    "                         snake=snake)\n",
    "\n",
    "grid = Grid(width=width, height=height)\n",
    "algorithm = Astar(\n",
    ")  #Objet qui ne servira à rien, uniquement pour l'instanciation de gameengine\n",
    "gameengine = GameEngine(snake=snake,\n",
    "                        rabbits=rabbits,\n",
    "                        grid=grid,\n",
    "                        algorithm=algorithm,\n",
    "                        one_rabbit_mode=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1726097774.770502  141287 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-12 01:36:14.795153: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2343] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function run_episode at 0x757af1de6980> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: \"set_loop_options\" must be the first statement in the loop block\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function run_episode at 0x757af1de6980> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: \"set_loop_options\" must be the first statement in the loop block\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OperatorNotAllowedInGraphError",
     "evalue": "in user code:\n\n    File \"/tmp/ipykernel_141287/2632671524.py\", line 7, in train_step  *\n        action_probs, values, rewards = run_episode(\n    File \"/tmp/ipykernel_141287/2451937718.py\", line 17, in run_episode\n        while not done:\n\n    OperatorNotAllowedInGraphError: Using a symbolic `tf.Tensor` as a Python `bool` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m t:\n\u001b[1;32m     20\u001b[0m     initial_state \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant(gameengine\u001b[38;5;241m.\u001b[39mget_state_tensor(),\n\u001b[1;32m     21\u001b[0m                                 dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mint32)\n\u001b[1;32m     22\u001b[0m     episode_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\n\u001b[0;32m---> 23\u001b[0m         \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgameengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                   \u001b[49m\u001b[43minitial_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                   \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     29\u001b[0m     episodes_reward\u001b[38;5;241m.\u001b[39mappend(episode_reward)\n\u001b[1;32m     30\u001b[0m     running_reward \u001b[38;5;241m=\u001b[39m statistics\u001b[38;5;241m.\u001b[39mmean(episodes_reward)\n",
      "File \u001b[0;32m~/projects/snakegame/venv/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/projects/snakegame/venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/autograph_util.py:52\u001b[0m, in \u001b[0;36mpy_func_from_autograph.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m     51\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[1;32m     53\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m: in user code:\n\n    File \"/tmp/ipykernel_141287/2632671524.py\", line 7, in train_step  *\n        action_probs, values, rewards = run_episode(\n    File \"/tmp/ipykernel_141287/2451937718.py\", line 17, in run_episode\n        while not done:\n\n    OperatorNotAllowedInGraphError: Using a symbolic `tf.Tensor` as a Python `bool` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.\n"
     ]
    }
   ],
   "source": [
    "min_episodes_criterion = 100\n",
    "max_episodes = 10000\n",
    "max_steps_per_episode = 1000\n",
    "\n",
    "# Cartpole-v0 is considered solved if average reward is >= 195 over 100\n",
    "# consecutive trials\n",
    "reward_threshold = 195\n",
    "running_reward = 0\n",
    "\n",
    "# Discount factor for future rewards\n",
    "gamma = 0.99\n",
    "optimizer = tf.keras.optimizers.Adam(0.01)\n",
    "\n",
    "# Keep last episodes reward\n",
    "episodes_reward: collections.deque = collections.deque(\n",
    "    maxlen=min_episodes_criterion)\n",
    "\n",
    "with tqdm.trange(max_episodes) as t:\n",
    "    for i in t:\n",
    "        initial_state = tf.constant(gameengine.get_state_tensor(),\n",
    "                                    dtype=tf.int32)\n",
    "        episode_reward = int(\n",
    "            train_step(env=gameengine,\n",
    "                       initial_state=initial_state,\n",
    "                       model=model,\n",
    "                       optimizer=optimizer,\n",
    "                       gamma=gamma))\n",
    "\n",
    "        episodes_reward.append(episode_reward)\n",
    "        running_reward = statistics.mean(episodes_reward)\n",
    "\n",
    "        t.set_description(f'Episode {i}')\n",
    "        t.set_postfix(episode_reward=episode_reward,\n",
    "                      running_reward=running_reward)\n",
    "\n",
    "        # Show average episode reward every 10 episodes\n",
    "        if i % 10 == 0:\n",
    "            pass  # print(f'Episode {i}: average reward: {avg_reward}')\n",
    "\n",
    "        if running_reward > reward_threshold and i >= min_episodes_criterion:\n",
    "            break\n",
    "\n",
    "print(f'\\nSolved at episode {i}: average reward: {running_reward:.2f}!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Astar' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m keras\u001b[38;5;241m.\u001b[39msaving\u001b[38;5;241m.\u001b[39msave_model(\u001b[43malgorithm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Astar' object has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "keras.saving.save_model(algorithm.model, \"model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train from existing model\n",
    "model = keras.saving.load_model(\"model.keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
